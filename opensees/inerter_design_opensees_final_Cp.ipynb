{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control import lqr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from matplotlib import font_manager  #matplotlib中 中文设置模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 输出高清图像\n",
    "% config InlineBackend.figure_format = 'retina'\n",
    "% matplotlib inline\n",
    "import platform\n",
    "# 图像显示中文的问题，需要判断系统是windows还是苹果的\n",
    "import matplotlib\n",
    "import platform\n",
    "\n",
    "sys_platform = platform.platform().lower()\n",
    "if \"windows\" in sys_platform:\n",
    "    font = {\n",
    "        \"family\": \"Times New Roman\"\n",
    "    }\n",
    "    matplotlib.rc(\"font\", **font)\n",
    "else:\n",
    "    font = {\n",
    "        \"family\": \"Arial Unicode MS\"\n",
    "    }\n",
    "    matplotlib.rc(\"font\", **font)\n",
    "rc = {\"mathtext.fontset\": \"stix\", }\n",
    "\n",
    "plt.rcParams.update(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(model, feature_names, X, y):\n",
    "    \"\"\"\n",
    "    筛选出15+27个重要特征\n",
    "    :param model: 能够计算特征重要性的模型\n",
    "    :param feature_names: 特征名\n",
    "    :param X: 训练集的输入数据\n",
    "    :param y: 训练集的输出数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 必须包含全部结构设计特征\n",
    "    feature_importances = 0\n",
    "    design_label = [\"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\",\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"]\n",
    "    length = len(feature_names)\n",
    "    design_num = 27\n",
    "    eq_num = 15\n",
    "    total_num = design_num + eq_num\n",
    "    while length > total_num:\n",
    "        print(length)\n",
    "        model.fit(X, y)\n",
    "        feature_importances = model.feature_importances_\n",
    "        indices = np.argsort(feature_importances)  # 下标排序\n",
    "        indices_flip = indices[::-1]  # 倒序\n",
    "        count1 = 0 # 统计结构设计特征\n",
    "        count2 = 0 # 统计地震特征\n",
    "        leave = []\n",
    "        if length >= total_num + 5:\n",
    "            idx = 0\n",
    "            while count1 < design_num and count2 < length - design_num - 5:\n",
    "                if feature_names[indices_flip[idx]] in design_label:\n",
    "                    leave.append(indices_flip[idx])\n",
    "                    count1 = count1 + 1\n",
    "                else:\n",
    "                    if count2 < length - design_num - 5:\n",
    "                        leave.append(indices_flip[idx])\n",
    "                        count2 = count2 + 1\n",
    "                idx = idx + 1\n",
    "        else:\n",
    "            idx = 0\n",
    "            while count1 < design_num and count2 < length - design_num - 1:\n",
    "                if feature_names[indices_flip[idx]] in design_label:\n",
    "                    leave.append(indices_flip[idx])\n",
    "                    count1 = count1 + 1\n",
    "                else:\n",
    "                    if count2 < length - design_num - 1:\n",
    "                        leave.append(indices_flip[idx])\n",
    "                        count2 = count2 + 1\n",
    "                idx = idx + 1\n",
    "        X = [X[:, i] for i in leave]\n",
    "        X = np.array(X).T\n",
    "        feature_names = [feature_names[i] for i in leave]\n",
    "        feature_importances = [feature_importances[i] for i in leave]\n",
    "        length = len(feature_names)\n",
    "    return feature_names, feature_importances\n",
    "\n",
    "\n",
    "# 特征排序\n",
    "def plot_features_importances(feature_importances, feature_names):\n",
    "    \"\"\"\n",
    "    特征重要性排序，选出占重要性排序前90%的特征\n",
    "    :param feature_importances:  特征重要性\n",
    "    :param feature_names: 特征名称\n",
    "    :return: 主要的特征\n",
    "    \"\"\"\n",
    "    length = len(feature_names)\n",
    "    # 渐变色\n",
    "    orange = (253 / 255, 200 / 255, 100 / 255)\n",
    "    red = (255 / 255, 20 / 255, 60 / 255)\n",
    "    r_step = (red[0] - orange[0]) / (length - 1)\n",
    "    g_step = (red[1] - orange[1]) / (length - 1)\n",
    "    b_step = (red[2] - orange[2]) / (length - 1)\n",
    "\n",
    "    plt.figure(figsize=(5, 8))\n",
    "    feature_importances = feature_importances[::-1]\n",
    "    feature_names = feature_names[::-1]\n",
    "\n",
    "    alpha_step = (1 - 0.7) / (length - 1)\n",
    "\n",
    "    pos = np.arange(length)\n",
    "    for i in range(length):\n",
    "        plt.barh(pos[i], feature_importances[i], align=\"center\", alpha=0.7 + i * alpha_step,\n",
    "                 color=[orange[0] + i * r_step, orange[1] + i * g_step, orange[2] + i * b_step], height=0.6, zorder=2)\n",
    "\n",
    "    plt.ylim([pos[0] - 0.5, pos[-1] + 0.5])\n",
    "    plt.ylabel(\"排序\", fontname='SimSun', fontsize=15)\n",
    "    plt.xlabel(\"相对重要性\", fontname='SimSun', fontsize=15)\n",
    "    plt.yticks(pos, feature_names, fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.grid(zorder=0)\n",
    "    xlabel = feature_importances\n",
    "    ylabel = pos\n",
    "    for x1, y1 in zip(xlabel, ylabel):\n",
    "        x1 = np.around(x1, decimals=3)\n",
    "        plt.text(x1 + 0.0005, y1 - 0.25, '%.3f' % x1, fontsize=5)\n",
    "\n",
    "\n",
    "# 特征排序\n",
    "def plot_features_importances2(feature_importances, feature_names):\n",
    "    \"\"\"\n",
    "    特征重要性排序，选出占重要性排序前90%的特征\n",
    "    :param feature_importances:  特征重要性\n",
    "    :param feature_names: 特征名称\n",
    "    :return: 主要的特征\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 20))\n",
    "    indices = np.argsort(feature_importances)  # 下标排序\n",
    "    indices_flip = indices[::-1]  # 倒序\n",
    "    names = []\n",
    "    for f in range(len(feature_names)):\n",
    "        names.append(feature_names[indices[f]])\n",
    "        print(\"%2d) %-*s %f\" % (f + 1, 30, feature_names[indices_flip[f]], feature_importances[indices_flip[f]]))\n",
    "\n",
    "    # 取前90%重要性的数据\n",
    "    sum_importances = 0\n",
    "    threshold = 0\n",
    "    for i in range(len(feature_importances)):\n",
    "        sum_importances += feature_importances[indices[i]]\n",
    "        if sum_importances >= 0.05:\n",
    "            threshold = i\n",
    "            break\n",
    "\n",
    "    pos = np.arange(indices.shape[0])\n",
    "    plt.barh(pos[0:threshold], feature_importances[indices[0:threshold]], align=\"center\")\n",
    "    plt.barh(pos[threshold:], feature_importances[indices[threshold:]], align=\"center\", color=\"red\")\n",
    "    plt.ylim([pos[0] - 1, pos[-1] + 1])\n",
    "    plt.ylabel(\"排序\", fontname='SimSun', fontsize=10)\n",
    "    plt.xlabel(\"相对重要性\", fontname='SimSun', fontsize=10)\n",
    "    plt.yticks(pos, names, fontsize=5)\n",
    "    plt.xticks(fontsize=5)\n",
    "    xlabel = feature_importances[indices]\n",
    "    ylabel = pos\n",
    "    for x1, y1 in zip(xlabel, ylabel):\n",
    "        x1 = np.around(x1, decimals=3)\n",
    "        plt.text(x1 + 0.0005, y1 - 0.25, '%.3f' % x1, fontsize=5)\n",
    "    plt.show()\n",
    "    return names[threshold:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/opensees_design_res.csv')\n",
    "df.info()\n",
    "df['ok'] = True\n",
    "df.loc[df[\"drift_max\"] > 1 / 200, ['ok']] = False\n",
    "df.loc[df[\"a_max\"] > 0.35 * 9.8, ['ok']] = False\n",
    "df.sample(3)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df[[\"a_max\", \"a_avg\", \"drift_max\", \"drift_avg\"]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a_max 0-1 1-2 >2\n",
    "a_avg 0-1 1-2 >2\n",
    "drift_max 0-1 1-2 2-3 3-4 >4\n",
    "drift_avg 0-1 1-2 2-3 >3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"a_max\"] = df_res[\"a_max\"] / 9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"a_avg\"] = df_res[\"a_avg\"] / 9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"drift_avg\"] = df_res[\"drift_avg\"] * 100\n",
    "df_res[\"drift_max\"] = df_res[\"drift_max\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"a_max_range\"] = pd.cut(x=df_res[\"a_max\"], bins=[0, 1, 2, 3])\n",
    "df_res[\"a_avg_range\"] = pd.cut(x=df_res[\"a_avg\"], bins=[0, 1, 2, 3])\n",
    "df_res[\"drift_max_range\"] = pd.cut(x=df_res[\"drift_max\"], bins=[0, 1, 2, 3, 4, 5])\n",
    "df_res[\"drift_avg_range\"] = pd.cut(x=df_res[\"drift_avg\"], bins=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"drift_avg_range\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = (255 / 255, 25 / 255, 65 / 255)\n",
    "blue = (52 / 255, 168 / 255, 255 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "val = df_res[\"a_max_range\"].value_counts()\n",
    "column_name = []\n",
    "for x in val.index.values:\n",
    "    column_name.append(x.__str__())\n",
    "column_name[-1] = column_name[-1][0:4] + \"+$\\infty $)\"\n",
    "plt.xlabel(r'数量', fontname='SimSun', fontsize=12)\n",
    "plt.ylabel(r'${{a}_{\\max }}$(g)', fontname='SimSun', fontsize=12)\n",
    "plt.barh(column_name, val.values, height=0.5, zorder=2, color=red)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(zorder=0)\n",
    "pic_name = r\"a_max分布图\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "val = df_res[\"a_avg_range\"].value_counts()\n",
    "column_name = []\n",
    "for x in val.index.values:\n",
    "    column_name.append(x.__str__())\n",
    "column_name[-1] = column_name[-1][0:4] + \"+$\\infty $)\"\n",
    "plt.xlabel(r'数量', fontname='SimSun', fontsize=12)\n",
    "plt.ylabel(r'${{a}_{avg}}$(g)', fontname='SimSun', fontsize=12)\n",
    "plt.barh(column_name, val.values, height=0.5, zorder=2, color=red)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(zorder=0)\n",
    "pic_name = r\"a_avg分布图\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "val = df_res[\"drift_max_range\"].value_counts()\n",
    "column_name = []\n",
    "for x in val.index.values:\n",
    "    column_name.append(x.__str__())\n",
    "column_name[-1] = column_name[-1][0:4] + \"+$\\infty $)\"\n",
    "plt.xlabel(r'数量', fontname='SimSun', fontsize=12)\n",
    "plt.ylabel(r'${{\\theta }_{\\max }}$(%)', fontname='SimSun', fontsize=12)\n",
    "plt.barh(column_name, val.values, height=0.5, zorder=2, color=blue)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(zorder=0)\n",
    "pic_name = r\"theata_max分布图\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "val = df_res[\"drift_avg_range\"].value_counts()\n",
    "column_name = []\n",
    "for x in val.index.values:\n",
    "    column_name.append(x.__str__())\n",
    "column_name[-1] = column_name[-1][0:4] + \"+$\\infty $)\"\n",
    "plt.xlabel(r'数量', fontname='SimSun', fontsize=12)\n",
    "plt.ylabel(r'${{\\theta }_{avg}}$(%)', fontname='SimSun', fontsize=12)\n",
    "plt.barh(column_name, val.values, height=0.5, zorder=2, color=blue)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(zorder=0)\n",
    "pic_name = r\"theata_avg分布图\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 利用GridSearchCV选择最优参数\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉id,site_name,miu,zeta,kappa,gamma,alpha,\n",
    "x_label = [\"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\",\n",
    "           \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"sa1\", \"sa2\", \"sa3\", \"sa4\", \"sa5\", \"sa6\", \"sa7\", \"sa8\",\n",
    "           \"sa9\", \"sa10\", \"sa11\", \"sa12\", \"sa13\", \"sa14\", \"sa15\", \"sa16\", \"sa17\", \"sa18\", \"sa19\", \"sa20\", \"sa21\",\n",
    "           \"sa22\", \"sa23\", \"sa24\", \"sa25\", \"sa26\", \"sa27\", \"sa28\", \"sa29\", \"sa30\", \"sv1\", \"sv2\", \"sv3\", \"sv4\", \"sv5\",\n",
    "           \"sv6\", \"sv7\", \"sv8\", \"sv9\", \"sv10\", \"sv11\", \"sv12\", \"sv13\", \"sv14\", \"sv15\", \"sv16\", \"sv17\", \"sv18\", \"sv19\",\n",
    "           \"sv20\", \"sv21\", \"sv22\", \"sv23\", \"sv24\", \"sv25\", \"sv26\", \"sv27\", \"sv28\", \"sv29\", \"sv30\", \"sd1\", \"sd2\", \"sd3\",\n",
    "           \"sd4\", \"sd5\", \"sd6\", \"sd7\", \"sd8\", \"sd9\", \"sd10\", \"sd11\", \"sd12\", \"sd13\", \"sd14\", \"sd15\", \"sd16\", \"sd17\",\n",
    "           \"sd18\", \"sd19\", \"sd20\", \"sd21\", \"sd22\", \"sd23\", \"sd24\", \"sd25\", \"sd26\", \"sd27\", \"sd28\", \"sd29\", \"sd30\",\n",
    "           \"sa_max\", \"sa_avg\", \"sv_max\", \"sv_avg\", \"sd_max\", \"sd_avg\", \"pga\", \"pgv\", \"pgd\", \"epa\", \"epv\", \"epd\", \"pa\",\n",
    "           \"pv\", \"pd\", \"ic\"]\n",
    "design_label = [\"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\",\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"]\n",
    "y_label = [\"ok\"]\n",
    "X = df.loc[:, x_label].values\n",
    "y = df.loc[:, y_label].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2)\n",
    "# y_test = y_test.reshape([-1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle_index = np.random.permutation(934)\n",
    "# X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = X_train\n",
    "new_X_test = X_test\n",
    "\n",
    "# normalizer = Normalizer(copy=True, norm='l2').fit(new_X_train)\n",
    "# ss = StandardScaler().fit(new_X_train)\n",
    "# new_X_train = ss.transform(new_X_train)\n",
    "# new_X_test = ss.transform(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贝叶斯优化\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = {'max_depth': np.arange(start=10, stop=500, step=10, dtype=int),\n",
    "              'n_estimators': np.arange(start=50, stop=1000, step=50, dtype=int),\n",
    "              'max_features': np.arange(start=1, stop=12, step=1, dtype=int),\n",
    "              'min_samples_leaf': np.array([2, 5, 10, 15], dtype=int),\n",
    "              'min_samples_split': np.array([2, 5, 10, 15], dtype=int),\n",
    "              'random_state': [3]}\n",
    "\n",
    "opt = BayesSearchCV(rfc, param_grid, n_iter=5, n_jobs=5, cv=3)\n",
    "\n",
    "opt.fit(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.best_params_)\n",
    "best_params = opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ok\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=best_params[\"n_estimators\"], max_features=best_params['max_features'],\n",
    "                             max_depth=best_params[\"max_depth\"], min_samples_split=best_params[\"min_samples_split\"],\n",
    "                             min_samples_leaf=best_params[\"min_samples_leaf\"], random_state=best_params[\"random_state\"])\n",
    "rfc.fit(new_X_train, y_train)\n",
    "y_pre_rfc = rfc.predict(new_X_test)\n",
    "\n",
    "print('正确标签：', y_test)\n",
    "print('预测结果：', y_pre_rfc)\n",
    "\n",
    "print('训练集分数：', rfc.score(new_X_train, y_train))\n",
    "print('测试集分数：', rfc.score(new_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "conf_mat = confusion_matrix(y_test, y_pre_rfc)\n",
    "plt.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    plt.text(j, i, '{0:d}'.format(z), ha='center', va='center')\n",
    "plt.colorbar()\n",
    "plt.ylabel('真实结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xlabel('预测结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "plt.yticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "pic_name = r\"随机森林分类混淆矩阵\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('混淆矩阵：')\n",
    "print(conf_mat)\n",
    "\n",
    "print('混淆矩阵：')\n",
    "print(conf_mat)\n",
    "\n",
    "# 分类指标文本报告（精确率、召回率、F1值等）\n",
    "print('分类指标报告：')\n",
    "print(classification_report(y_test, y_pre_rfc))\n",
    "\n",
    "# 特征重要性\n",
    "print(rfc.feature_importances_)\n",
    "\n",
    "# 画图展示训练结果\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# f2 = ax.scatter(list(range(len(X_test))), y_pre_rfc, marker='o')\n",
    "# f1 = ax.scatter(list(range(len(X_test))), y_test, marker='*')\n",
    "# plt.legend(handles=[f1, f2], labels=['True', 'Prediction'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(model, feature_names, X, y):\n",
    "    \"\"\"\n",
    "    筛选出15+27个重要特征\n",
    "    :param model: 能够计算特征重要性的模型\n",
    "    :param feature_names: 特征名\n",
    "    :param X: 训练集的输入数据\n",
    "    :param y: 训练集的输出数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 必须包含全部结构设计特征\n",
    "    feature_importances = 0\n",
    "    design_label = [\"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\",\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"]\n",
    "    length = len(feature_names)\n",
    "    design_num = 27\n",
    "    eq_num = 15\n",
    "    total_num = design_num + eq_num\n",
    "    while length > total_num:\n",
    "        print(length)\n",
    "        model.fit(X, y)\n",
    "        feature_importances = model.feature_importances_\n",
    "        indices = np.argsort(feature_importances)  # 下标排序\n",
    "        indices_flip = indices[::-1]  # 倒序\n",
    "        count1 = 0 # 统计结构设计特征\n",
    "        count2 = 0 # 统计地震特征\n",
    "        leave = []\n",
    "        if length >= total_num + 5:\n",
    "            idx = 0\n",
    "            while count1 < design_num or count2 < length - design_num - 5:\n",
    "                if feature_names[indices_flip[idx]] in design_label:\n",
    "                    leave.append(indices_flip[idx])\n",
    "                    count1 = count1 + 1\n",
    "                else:\n",
    "                    if count2 < length - design_num - 5:\n",
    "                        leave.append(indices_flip[idx])\n",
    "                        count2 = count2 + 1\n",
    "                idx = idx + 1\n",
    "        else:\n",
    "            idx = 0\n",
    "            while count1 < design_num or count2 < length - design_num - 1:\n",
    "                if feature_names[indices_flip[idx]] in design_label:\n",
    "                    leave.append(indices_flip[idx])\n",
    "                    count1 = count1 + 1\n",
    "                else:\n",
    "                    if count2 < length - design_num - 1:\n",
    "                        leave.append(indices_flip[idx])\n",
    "                        count2 = count2 + 1\n",
    "                idx = idx + 1\n",
    "        X = [X[:, i] for i in leave]\n",
    "        X = np.array(X).T\n",
    "        feature_names = [feature_names[i] for i in leave]\n",
    "        feature_importances = [feature_importances[i] for i in leave]\n",
    "        length = len(feature_names)\n",
    "    return feature_names, feature_importances\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=best_params[\"n_estimators\"], max_features=best_params['max_features'],\n",
    "                             max_depth=best_params[\"max_depth\"], min_samples_split=best_params[\"min_samples_split\"],\n",
    "                             min_samples_leaf=best_params[\"min_samples_leaf\"], random_state=best_params[\"random_state\"])\n",
    "main_features, feature_importances = select_features(rfc, x_label, new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importances(feature_importances, main_features)\n",
    "pic_name = r\"随机森林重要特征排序\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "main_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用主特征\n",
    "X_main = df.loc[:, main_features].values\n",
    "X_main_train, X_main_test, y_train, y_test = train_test_split(X_main, y, test_size=0.1, random_state=2)\n",
    "print(X_main_train.shape, X_main_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "new_X_main_train = X_main_train\n",
    "new_X_main_test = X_main_test\n",
    "\n",
    "# ss_main = StandardScaler().fit(new_X_main_train)\n",
    "# new_X_main_train = ss_main.transform(new_X_main_train)\n",
    "# new_X_main_test = ss_main.transform(new_X_main_test)\n",
    "new_X_main_test1 = new_X_main_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=best_params[\"n_estimators\"], max_features=best_params['max_features'],\n",
    "                             max_depth=best_params[\"max_depth\"], min_samples_split=best_params[\"min_samples_split\"],\n",
    "                             min_samples_leaf=best_params[\"min_samples_leaf\"], random_state=best_params[\"random_state\"])\n",
    "rfc.fit(new_X_main_train, y_train)\n",
    "y_pre_rfc = rfc.predict(new_X_main_test)\n",
    "print('正确标签：', y_test)\n",
    "print('预测结果：', y_pre_rfc)\n",
    "\n",
    "print('训练集分数：', rfc.score(new_X_main_train, y_train))\n",
    "print('测试集分数：', rfc.score(new_X_main_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "conf_mat = confusion_matrix(y_test, y_pre_rfc)\n",
    "plt.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    plt.text(j, i, '{0:d}'.format(z), ha='center', va='center')\n",
    "plt.colorbar()\n",
    "plt.ylabel('真实结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xlabel('预测结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "plt.yticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "pic_name = r\"随机森林分类混淆矩阵_优化\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('混淆矩阵：')\n",
    "print(conf_mat)\n",
    "\n",
    "print('混淆矩阵：')\n",
    "print(conf_mat)\n",
    "\n",
    "# 分类指标文本报告（精确率、召回率、F1值等）\n",
    "print('分类指标报告：')\n",
    "print(classification_report(y_test, y_pre_rfc))\n",
    "\n",
    "# 特征重要性\n",
    "print(rfc.feature_importances_)\n",
    "\n",
    "# # 画图展示训练结果\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# f2 = ax.scatter(list(range(len(X_test))), y_pre_rfc, marker='o')\n",
    "# f1 = ax.scatter(list(range(len(X_test))), y_test, marker='*')\n",
    "# plt.legend(handles=[f1, f2], labels=['True', 'Prediction'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_main_train[0].shape\n",
    "rfc.predict(new_X_main_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rfc, \"./ml_models/rfc.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贝叶斯优化\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# 贝叶斯优化\n",
    "xgBoostC = XGBClassifier()\n",
    "\n",
    "param_grid = {'learning_rate': np.array([0.01, 0.015, 0.025, 0.05, 0.1], dtype=float),\n",
    "              'gamma': np.array([0, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1], dtype=float),\n",
    "              \"reg_alpha\": np.array([0, 0.01, 0.1, 1], dtype=float),\n",
    "              \"reg_lambda\": np.array([0, 0.1, 0.5, 1], dtype=float),\n",
    "              \"min_child_weight\": np.array([1, 3, 5, 7], dtype=int),\n",
    "              'subsample': np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1], dtype=float),\n",
    "              'max_depth': np.array([3, 5, 8, 15, 25, 30], dtype=int),\n",
    "              'n_estimators': np.arange(start=50, stop=3000, step=50, dtype=int),\n",
    "              'random_state': [3]}\n",
    "\n",
    "opt = BayesSearchCV(xgBoostC, param_grid, n_iter=50, n_jobs=30, cv=3)\n",
    "\n",
    "opt.fit(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.best_params_)\n",
    "best_params = opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoostC = XGBClassifier(n_estimators=best_params[\"n_estimators\"],\n",
    "                         max_depth=best_params[\"max_depth\"],\n",
    "                         learning_rate=best_params[\"learning_rate\"],\n",
    "                         gamma=best_params[\"gamma\"],\n",
    "                         min_child_weight=best_params[\"min_child_weight\"],\n",
    "                         reg_alpha=best_params[\"reg_alpha\"],\n",
    "                         reg_lambda=best_params[\"reg_lambda\"],\n",
    "                         subsample=best_params[\"subsample\"],\n",
    "                         random_state=best_params[\"random_state\"])\n",
    "xgBoostC.fit(new_X_train, y_train)\n",
    "y_pre_xgBoostC = xgBoostC.predict(new_X_test)\n",
    "\n",
    "print('正确标签：', y_test)\n",
    "print('预测结果：', y_pre_xgBoostC)\n",
    "\n",
    "print('训练集分数：', xgBoostC.score(new_X_train, y_train))\n",
    "print('测试集分数：', xgBoostC.score(new_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "conf_mat = confusion_matrix(y_test, y_pre_xgBoostC)\n",
    "plt.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    plt.text(j, i, '{0:d}'.format(z), ha='center', va='center')\n",
    "plt.colorbar()\n",
    "plt.ylabel('真实结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xlabel('预测结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "plt.yticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "pic_name = r\"XGBoost分类混淆矩阵\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('混淆矩阵：')\n",
    "print(conf_mat)\n",
    "\n",
    "# 分类指标文本报告（精确率、召回率、F1值等）\n",
    "print('分类指标报告：')\n",
    "print(classification_report(y_test, y_pre_xgBoostC))\n",
    "\n",
    "# 特征重要性\n",
    "print(xgBoostC.feature_importances_)\n",
    "\n",
    "# # 画图展示训练结果\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# f2 = ax.scatter(list(range(len(X_test))), y_pre_xgBoostC, marker='o')\n",
    "# f1 = ax.scatter(list(range(len(X_test))), y_test, marker='*')\n",
    "# plt.legend(handles=[f1, f2], labels=['True', 'Prediction'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoostC = XGBClassifier(n_estimators=best_params[\"n_estimators\"],\n",
    "                         max_depth=best_params[\"max_depth\"],\n",
    "                         learning_rate=best_params[\"learning_rate\"],\n",
    "                         gamma=best_params[\"gamma\"],\n",
    "                         min_child_weight=best_params[\"min_child_weight\"],\n",
    "                         reg_alpha=best_params[\"reg_alpha\"],\n",
    "                         reg_lambda=best_params[\"reg_lambda\"],\n",
    "                         subsample=best_params[\"subsample\"],\n",
    "                         random_state=best_params[\"random_state\"])\n",
    "main_features2, feature_importances2 = select_features(xgBoostC, x_label, new_X_train, y_train)\n",
    "plot_features_importances(feature_importances2, main_features2)\n",
    "pic_name = r\"XGBoost重要特征排序\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用主特征\n",
    "X_main = df.loc[:, main_features2].values\n",
    "X_main_train, X_main_test, y_train, y_test = train_test_split(X_main, y, test_size=0.1, random_state=2)\n",
    "print(X_main_train.shape, X_main_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "new_X_main_train = X_main_train\n",
    "new_X_main_test = X_main_test\n",
    "\n",
    "ss_main = StandardScaler().fit(new_X_main_train)\n",
    "new_X_main_train = ss_main.transform(new_X_main_train)\n",
    "new_X_main_test = ss_main.transform(new_X_main_test)\n",
    "new_X_main_test2 = new_X_main_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoostC = XGBClassifier(n_estimators=best_params[\"n_estimators\"],\n",
    "                         max_depth=best_params[\"max_depth\"],\n",
    "                         learning_rate=best_params[\"learning_rate\"],\n",
    "                         gamma=best_params[\"gamma\"],\n",
    "                         min_child_weight=best_params[\"min_child_weight\"],\n",
    "                         reg_alpha=best_params[\"reg_alpha\"],\n",
    "                         reg_lambda=best_params[\"reg_lambda\"],\n",
    "                         subsample=best_params[\"subsample\"],\n",
    "                         random_state=best_params[\"random_state\"])\n",
    "xgBoostC.fit(new_X_main_train, y_train)\n",
    "y_pre_xgBoostC = xgBoostC.predict(new_X_main_test)\n",
    "\n",
    "print('正确标签：', y_test)\n",
    "print('预测结果：', y_pre_xgBoostC)\n",
    "\n",
    "print('训练集分数：', xgBoostC.score(new_X_main_train, y_train))\n",
    "print('测试集分数：', xgBoostC.score(new_X_main_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "conf_mat = confusion_matrix(y_test, y_pre_xgBoostC)\n",
    "plt.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    plt.text(j, i, '{0:d}'.format(z), ha='center', va='center')\n",
    "plt.colorbar()\n",
    "plt.ylabel('真实结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xlabel('预测结果', fontname=\"SimSun\", fontsize=12)\n",
    "plt.xticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "plt.yticks([0, 1], [\"False\", \"True\"], fontsize=10)\n",
    "pic_name = r\"XGBoost分类混淆矩阵\"\n",
    "plt.savefig(r'./pic/{}.png'.format(pic_name), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('混淆矩阵：')\n",
    "print(conf_mat)\n",
    "\n",
    "# 分类指标文本报告（精确率、召回率、F1值等）\n",
    "print('分类指标报告：')\n",
    "print(classification_report(y_test, y_pre_xgBoostC))\n",
    "\n",
    "# 特征重要性\n",
    "print(xgBoostC.feature_importances_)\n",
    "\n",
    "# # 画图展示训练结果\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# f2 = ax.scatter(list(range(len(X_test))), y_pre_xgBoostC, marker='o')\n",
    "# f1 = ax.scatter(list(range(len(X_test))), y_test, marker='*')\n",
    "# plt.legend(handles=[f1, f2], labels=['True', 'Prediction'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制ROC曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 输出高清图像\n",
    "% config InlineBackend.figure_format = 'retina'\n",
    "% matplotlib inline\n",
    "import platform\n",
    "## 图像显示中文的问题，需要判断系统是windows还是苹果的\n",
    "import matplotlib\n",
    "import platform\n",
    "\n",
    "sys_platform = platform.platform().lower()\n",
    "if \"windows\" in sys_platform:\n",
    "    font = {\n",
    "        \"family\": \"Microsoft YaHei\"\n",
    "    }\n",
    "    matplotlib.rc(\"font\", **font)\n",
    "else:\n",
    "    font = {\n",
    "        \"family\": \"Arial Unicode MS\"\n",
    "    }\n",
    "    matplotlib.rc(\"font\", **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# 可视化在验证集上的Roc曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plot_roc_curve(rfc, new_X_main_test1, y_test, ax=ax)\n",
    "plot_roc_curve(xgBoostC, new_X_main_test2, y_test, ax=ax)\n",
    "# plot_roc_curve(knc, new_X_test, y_test, ax=ax)\n",
    "# stacking模型\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, stac.decision_function(new_X_test))\n",
    "# stack_auc = roc_auc_score(y_test, stac.decision_function(new_X_test))\n",
    "# plt.plot(fpr, tpr, label=\"Stacking(AUC={:.2f})\".format(stack_auc))\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"假正率\", fontname='SimSun', fontsize=10)\n",
    "plt.ylabel(\"真正率\", fontname='SimSun', fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "# plt.title(\"ROC曲线\", fontname='SimSun', fontsize=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_main_train[0].shape\n",
    "rfc.predict(new_X_main_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rfc, \"./ml_models/rfc.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label = [\"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\",\n",
    "           \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"sa1\", \"sa2\", \"sa3\", \"sa4\", \"sa5\", \"sa6\", \"sa7\", \"sa8\",\n",
    "           \"sa9\", \"sa10\", \"sa11\", \"sa12\", \"sa13\", \"sa14\", \"sa15\", \"sa16\", \"sa17\", \"sa18\", \"sa19\", \"sa20\", \"sa21\",\n",
    "           \"sa22\", \"sa23\", \"sa24\", \"sa25\", \"sa26\", \"sa27\", \"sa28\", \"sa29\", \"sa30\", \"sv1\", \"sv2\", \"sv3\", \"sv4\", \"sv5\",\n",
    "           \"sv6\", \"sv7\", \"sv8\", \"sv9\", \"sv10\", \"sv11\", \"sv12\", \"sv13\", \"sv14\", \"sv15\", \"sv16\", \"sv17\", \"sv18\", \"sv19\",\n",
    "           \"sv20\", \"sv21\", \"sv22\", \"sv23\", \"sv24\", \"sv25\", \"sv26\", \"sv27\", \"sv28\", \"sv29\", \"sv30\", \"sd1\", \"sd2\", \"sd3\",\n",
    "           \"sd4\", \"sd5\", \"sd6\", \"sd7\", \"sd8\", \"sd9\", \"sd10\", \"sd11\", \"sd12\", \"sd13\", \"sd14\", \"sd15\", \"sd16\", \"sd17\",\n",
    "           \"sd18\", \"sd19\", \"sd20\", \"sd21\", \"sd22\", \"sd23\", \"sd24\", \"sd25\", \"sd26\", \"sd27\", \"sd28\", \"sd29\", \"sd30\",\n",
    "           \"sa_max\", \"sa_avg\", \"sv_max\", \"sv_avg\", \"sd_max\", \"sd_avg\", \"pga\", \"pgv\", \"pgd\", \"epa\", \"epv\", \"epd\", \"pa\",\n",
    "           \"pv\", \"pd\", \"ic\"]\n",
    "\n",
    "y_label = [\"drift_max\", \"drift_avg\", \"a_max\", \"a_avg\", \"T_1\"]\n",
    "\n",
    "df_ok_origin = df[df.ok == True]\n",
    "df_ok = df_ok_origin.copy(deep=True)\n",
    "df_ok.describe()\n",
    "X = df_ok.loc[:, x_label].values\n",
    "y = df_ok.loc[:, y_label].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "y_train_theta_max = y_train[:, 0]\n",
    "y_test_theta_max = y_test[:, 0]\n",
    "y_train_theta_avg = y_train[:, 1]\n",
    "y_test_theta_avg = y_test[:, 1]\n",
    "y_train_a_max = y_train[:, 2]\n",
    "y_test_a_max = y_test[:, 2]\n",
    "y_train_a_avg = y_train[:, 3]\n",
    "y_test_a_avg = y_test[:, 3]\n",
    "# y_train_T1 = y_train[:, 4]\n",
    "# y_test_T1 = y_test[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征标准化，采用最大最小值标准化，转化后的值范围（0,1）\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# min_max_scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "new_X_train = X_train\n",
    "new_X_test = X_test\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# normalizer = Normalizer(copy=True, norm='l2').fit(new_X_train)\n",
    "# new_X_train = normalizer.transform(new_X_train)\n",
    "# # new_X_test = normalizer.transform(new_X_test)\n",
    "# ss = StandardScaler().fit(new_X_train)\n",
    "# new_X_train = ss.transform(new_X_train)\n",
    "# new_X_test = ss.transform(new_X_test)\n",
    "\n",
    "# new_X_T1_train = new_X_train[:, 0: 27]\n",
    "# new_X_T1_test = new_X_test[:, 0: 27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_theta_max = RandomForestRegressor()\n",
    "rfr_theta_avg = RandomForestRegressor()\n",
    "rfr_a_max = RandomForestRegressor()\n",
    "rfr_a_avg = RandomForestRegressor()\n",
    "\n",
    "# 贝叶斯优化\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "param_grid = {'max_depth': np.arange(start=10, stop=500, step=10, dtype=int),\n",
    "              'n_estimators': np.arange(start=50, stop=1000, step=50, dtype=int),\n",
    "              'max_features': np.arange(start=1, stop=12, step=1, dtype=int),\n",
    "              'min_samples_leaf': np.array([2, 5, 10, 15], dtype=int),\n",
    "              'min_samples_split': np.array([2, 5, 10, 15], dtype=int),\n",
    "              'random_state': [3]}\n",
    "opt_theta_max = BayesSearchCV(rfr_theta_max, param_grid, n_iter=5, n_jobs=5, cv=3)\n",
    "opt_theta_avg = BayesSearchCV(rfr_theta_avg, param_grid, n_iter=5, n_jobs=5, cv=3)\n",
    "opt_a_max = BayesSearchCV(rfr_a_max, param_grid, n_iter=5, n_jobs=5, cv=3)\n",
    "opt_a_avg = BayesSearchCV(rfr_a_avg, param_grid, n_iter=5, n_jobs=5, cv=3)\n",
    "\n",
    "opt_theta_max.fit(new_X_train, y_train_theta_max)\n",
    "opt_theta_avg.fit(new_X_train, y_train_theta_avg)\n",
    "opt_a_max.fit(new_X_train, y_train_a_max)\n",
    "opt_a_avg.fit(new_X_train, y_train_a_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_theta_max = opt_theta_max.best_params_\n",
    "best_params_theta_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_theta_avg = opt_theta_avg.best_params_\n",
    "best_params_theta_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_a_max = opt_a_max.best_params_\n",
    "best_params_a_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_a_avg = opt_a_avg.best_params_\n",
    "best_params_a_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "\n",
    "def plot_regression_results(ax, y_true, y_pred, title, scores):\n",
    "    \"\"\"预测目标与真实目标的散点图。\"\"\"\n",
    "    ax.plot([y_true.min(), y_true.max()],\n",
    "            [y_true.min(), y_true.max()], '--r', linewidth=2)\n",
    "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    ax.set_xlim([y_true.min(), y_true.max()])\n",
    "    ax.set_ylim([y_true.min(), y_true.max()])\n",
    "    ax.set_xlabel('真实值', fontname='SimSun', fontsize=10)\n",
    "    ax.set_ylabel('预测值', fontname='SimSun', fontsize=10)\n",
    "    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n",
    "                          edgecolor='none', linewidth=0)\n",
    "    ax.legend([extra], [scores], loc='upper left')\n",
    "    ax.set_title(title, fontname='SimSun', fontsize=15)\n",
    "\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "def plot_true_predict_pic(y_true, y_pred, model_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    msle = mean_squared_log_error(y_true, y_pred)\n",
    "    rmsle = np.sqrt(msle)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    ax = plt.gca()\n",
    "    plot_regression_results(\n",
    "        ax, y_true, y_pred,\n",
    "        model_name,\n",
    "        (r'$R^2={:.2f}$' + '\\n' +\n",
    "         r'$MAE={:.2f}$' + '\\n' +\n",
    "         r'$RMSE={:.2f}$' + '\\n' +\n",
    "         r'$MAPE={:.2f}$' + '\\n' +\n",
    "         r'$RMSLE={:.2f}$')\n",
    "        .format(r2, mae, rmse, mape, rmsle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theta_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_theta_max = RandomForestRegressor(max_depth=best_params_theta_max[\"max_depth\"],\n",
    "                                      n_estimators=best_params_theta_max[\"n_estimators\"],\n",
    "                                      max_features=best_params_theta_max[\"max_features\"],\n",
    "                                      min_samples_leaf=best_params_theta_max[\"min_samples_leaf\"],\n",
    "                                      min_samples_split=best_params_theta_max[\"min_samples_split\"],\n",
    "                                      random_state=best_params_theta_max[\"random_state\"]\n",
    "                                      )\n",
    "rfr_theta_max.fit(new_X_train, y_train_theta_max)\n",
    "\n",
    "train_pred_theta_max = rfr_theta_max.predict(new_X_train)\n",
    "pred_theta_max = rfr_theta_max.predict(new_X_test)\n",
    "print(rmsle(y_train_theta_max, train_pred_theta_max))\n",
    "\n",
    "plot_true_predict_pic(y_test_theta_max, pred_theta_max, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选特征\n",
    "main_features_theta_max, feature_importances_theta_max = select_features(rfr_theta_max, x_label, new_X_train, y_train_theta_max)\n",
    "plot_features_importances(feature_importances_theta_max, main_features_theta_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用主特征\n",
    "X_main = df_ok.loc[:, main_features_theta_max].values\n",
    "X_main_train, X_main_test, y_train, y_test = train_test_split(X_main, y, test_size=0.1, random_state=2)\n",
    "print(X_main_train.shape, X_main_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "new_X_main_train = X_main_train\n",
    "new_X_main_test = X_main_test\n",
    "rfr_theta_max = RandomForestRegressor(max_depth=best_params_theta_max[\"max_depth\"],\n",
    "                                      n_estimators=best_params_theta_max[\"n_estimators\"],\n",
    "                                      min_samples_leaf=best_params_theta_max[\"min_samples_leaf\"],\n",
    "                                      min_samples_split=best_params_theta_max[\"min_samples_split\"],\n",
    "                                      random_state=best_params_theta_max[\"random_state\"]\n",
    "                                      )\n",
    "rfr_theta_max.fit(new_X_main_train, y_train_theta_max)\n",
    "\n",
    "train_pred_theta_max = rfr_theta_max.predict(new_X_main_train)\n",
    "pred_theta_max = rfr_theta_max.predict(new_X_main_test)\n",
    "print(rmsle(y_train_theta_max, train_pred_theta_max))\n",
    "\n",
    "plot_true_predict_pic(y_test_theta_max, pred_theta_max, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theta_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_theta_avg = RandomForestRegressor(max_depth=best_params_theta_avg[\"max_depth\"],\n",
    "                                      n_estimators=best_params_theta_avg[\"n_estimators\"],\n",
    "                                      max_features=best_params_theta_avg[\"max_features\"],\n",
    "                                      min_samples_leaf=best_params_theta_avg[\"min_samples_leaf\"],\n",
    "                                      min_samples_split=best_params_theta_avg[\"min_samples_split\"],\n",
    "                                      random_state=best_params_theta_avg[\"random_state\"]\n",
    "                                      )\n",
    "rfr_theta_avg.fit(new_X_train, y_train_theta_avg)\n",
    "\n",
    "train_pred_theta_avg = rfr_theta_avg.predict(new_X_train)\n",
    "pred_theta_avg = rfr_theta_avg.predict(new_X_test)\n",
    "print(rmsle(y_train_theta_avg, train_pred_theta_avg))\n",
    "\n",
    "plot_true_predict_pic(y_test_theta_avg, pred_theta_avg, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_features_importances2(rfr_theta_avg.feature_importances_, x_label)\n",
    "# 筛选主特征\n",
    "main_features_theta_avg, feature_importances_theta_avg = select_features(rfr_theta_avg, x_label, new_X_train, y_train_theta_avg)\n",
    "plot_features_importances(feature_importances_theta_avg, main_features_theta_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用主特征\n",
    "X_main = df_ok.loc[:, main_features_theta_avg].values\n",
    "X_main_train, X_main_test, y_train, y_test = train_test_split(X_main, y, test_size=0.1, random_state=2)\n",
    "print(X_main_train.shape, X_main_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "new_X_main_train = X_main_train\n",
    "new_X_main_test = X_main_test\n",
    "rfr_theta_avg = RandomForestRegressor(max_depth=best_params_theta_avg[\"max_depth\"],\n",
    "                                      n_estimators=best_params_theta_avg[\"n_estimators\"],\n",
    "                                      max_features=best_params_theta_avg[\"max_features\"],\n",
    "                                      min_samples_leaf=best_params_theta_avg[\"min_samples_leaf\"],\n",
    "                                      min_samples_split=best_params_theta_avg[\"min_samples_split\"],\n",
    "                                      random_state=best_params_theta_avg[\"random_state\"]\n",
    "                                      )\n",
    "rfr_theta_avg.fit(new_X_main_train, y_train_theta_avg)\n",
    "\n",
    "train_pred_theta_avg = rfr_theta_avg.predict(new_X_main_train)\n",
    "pred_theta_avg = rfr_theta_avg.predict(new_X_main_test)\n",
    "print(rmsle(y_train_theta_avg, train_pred_theta_avg))\n",
    "\n",
    "plot_true_predict_pic(y_test_theta_avg, pred_theta_avg, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_a_max = RandomForestRegressor(max_depth=best_params_a_max[\"max_depth\"],\n",
    "                                  n_estimators=best_params_a_max[\"n_estimators\"],\n",
    "                                  max_features=best_params_a_max[\"max_features\"],\n",
    "                                  min_samples_leaf=best_params_a_max[\"min_samples_leaf\"],\n",
    "                                  min_samples_split=best_params_a_max[\"min_samples_split\"],\n",
    "                                  random_state=best_params_a_max[\"random_state\"]\n",
    "                                  )\n",
    "rfr_a_max.fit(new_X_train, y_train_a_max)\n",
    "\n",
    "train_pred_a_max = rfr_a_max.predict(new_X_train)\n",
    "pred_a_max = rfr_a_max.predict(new_X_test)\n",
    "print(rmsle(y_train_a_max, train_pred_a_max))\n",
    "\n",
    "plot_true_predict_pic(y_test_a_max, pred_a_max, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_features_importances(rfr_a_max.feature_importances_, x_label)\n",
    "main_features_a_max, feature_importances_a_max = select_features(rfr_a_max, x_label, new_X_train, y_train_a_max)\n",
    "plot_features_importances(feature_importances_a_max, main_features_a_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用主特征\n",
    "X_main = df_ok.loc[:, main_features_a_max].values\n",
    "X_main_train, X_main_test, y_train, y_test = train_test_split(X_main, y, test_size=0.1, random_state=2)\n",
    "print(X_main_train.shape, X_main_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "new_X_main_train = X_main_train\n",
    "new_X_main_test = X_main_test\n",
    "rfr_a_max = RandomForestRegressor(max_depth=best_params_a_max[\"max_depth\"],\n",
    "                                  n_estimators=best_params_a_max[\"n_estimators\"],\n",
    "                                  max_features=best_params_a_max[\"max_features\"],\n",
    "                                  min_samples_leaf=best_params_a_max[\"min_samples_leaf\"],\n",
    "                                  min_samples_split=best_params_a_max[\"min_samples_split\"],\n",
    "                                  random_state=best_params_a_max[\"random_state\"]\n",
    "                                  )\n",
    "rfr_a_max.fit(new_X_main_train, y_train_a_max)\n",
    "\n",
    "train_pred_a_max = rfr_a_max.predict(new_X_main_train)\n",
    "pred_a_max = rfr_a_max.predict(new_X_main_test)\n",
    "print(rmsle(y_train_a_max, train_pred_a_max))\n",
    "\n",
    "plot_true_predict_pic(y_test_a_max, pred_a_max, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_a_avg = RandomForestRegressor(max_depth=best_params_a_avg[\"max_depth\"],\n",
    "                                  n_estimators=best_params_a_avg[\"n_estimators\"],\n",
    "                                  min_samples_leaf=best_params_a_avg[\"min_samples_leaf\"],\n",
    "                                  min_samples_split=best_params_a_avg[\"min_samples_split\"],\n",
    "                                  random_state=best_params_a_avg[\"random_state\"]\n",
    "                                  )\n",
    "rfr_a_avg.fit(new_X_train, y_train_a_avg)\n",
    "\n",
    "train_pred_a_avg = rfr_a_avg.predict(new_X_train)\n",
    "pred_a_avg = rfr_a_avg.predict(new_X_test)\n",
    "print(rmsle(y_train_a_avg, train_pred_a_avg))\n",
    "\n",
    "plot_true_predict_pic(y_test_a_avg, pred_a_avg, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选特征\n",
    "main_features_a_avg, feature_importances_a_avg = select_features(rfr_a_avg, x_label, new_X_train, y_train_a_avg)\n",
    "# feature_importances_a_avg_fake = [i * 2 for i in feature_importances_a_avg]\n",
    "# feature_importances_a_avg_fake[0] = feature_importances_a_avg[0]\n",
    "# plot_features_importances(feature_importances_a_avg_fake, main_features_a_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用主特征\n",
    "X_main = df_ok.loc[:, main_features_a_avg].values\n",
    "X_main_train, X_main_test, y_train, y_test = train_test_split(X_main, y, test_size=0.1, random_state=2)\n",
    "print(X_main_train.shape, X_main_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "new_X_main_train = X_main_train\n",
    "new_X_main_test = X_main_test\n",
    "rfr_a_avg = RandomForestRegressor(max_depth=best_params_a_avg[\"max_depth\"],\n",
    "                                  n_estimators=best_params_a_avg[\"n_estimators\"],\n",
    "                                  max_features=best_params_a_avg[\"max_features\"],\n",
    "                                  min_samples_leaf=best_params_a_avg[\"min_samples_leaf\"],\n",
    "                                  min_samples_split=best_params_a_avg[\"min_samples_split\"],\n",
    "                                  random_state=best_params_a_avg[\"random_state\"]\n",
    "                                  )\n",
    "rfr_a_avg.fit(new_X_main_train, y_train_a_avg)\n",
    "\n",
    "train_pred_a_avg = rfr_a_avg.predict(new_X_main_train)\n",
    "pred_a_avg = rfr_a_avg.predict(new_X_main_test)\n",
    "print(rmsle(y_train_a_avg, train_pred_a_avg))\n",
    "\n",
    "plot_true_predict_pic(y_test_a_avg, pred_a_avg, \"随机森林模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_features_a_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rfr_theta_max, \"./ml_models/rfr_theta_max.model\")\n",
    "joblib.dump(rfr_theta_avg, \"./ml_models/rfr_theta_avg.model\")\n",
    "joblib.dump(rfr_a_max, \"./ml_models/rfr_a_max.model\")\n",
    "joblib.dump(rfr_a_avg, \"./ml_models/rfr_a_avg.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_theta_max = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n",
    "                                       learning_rate=0.05, max_depth=3,\n",
    "                                       min_child_weight=1.7817, n_estimators=2200,\n",
    "                                       reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                                       subsample=0.5213, silent=1,\n",
    "                                       random_state=7, nthread=-1)\n",
    "model_xgb_theta_avg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n",
    "                                       learning_rate=0.05, max_depth=3,\n",
    "                                       min_child_weight=1.7817, n_estimators=2200,\n",
    "                                       reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                                       subsample=0.5213, silent=1,\n",
    "                                       random_state=7, nthread=-1)\n",
    "model_xgb_a_max = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n",
    "                                   learning_rate=0.05, max_depth=3,\n",
    "                                   min_child_weight=1.7817, n_estimators=2200,\n",
    "                                   reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                                   subsample=0.5213, silent=1,\n",
    "                                   random_state=7, nthread=-1)\n",
    "model_xgb_a_avg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n",
    "                                   learning_rate=0.05, max_depth=3,\n",
    "                                   min_child_weight=1.7817, n_estimators=2200,\n",
    "                                   reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                                   subsample=0.5213, silent=1,\n",
    "                                   random_state=7, nthread=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model_theta_max = GBoost_theta_max\n",
    "select_model_theta_avg = GBoost_theta_avg\n",
    "select_model_a_max = GBoost_a_max\n",
    "select_model_a_avg = GBoost_a_avg\n",
    "select_model_T1 = GBoost_T1\n",
    "\n",
    "select_model_theta_max.fit(new_X_train, y_train_theta_max)\n",
    "select_model_theta_avg.fit(new_X_train, y_train_theta_avg)\n",
    "select_model_a_max.fit(new_X_train, y_train_a_max)\n",
    "select_model_a_avg.fit(new_X_train, y_train_a_avg)\n",
    "select_model_T1.fit(new_X_T1_train, y_train_T1)\n",
    "\n",
    "train_pred_theta_max = select_model_theta_max.predict(new_X_train)\n",
    "pred_theta_max = select_model_theta_max.predict(new_X_test)\n",
    "print(rmsle(y_train_theta_max, train_pred_theta_max))\n",
    "\n",
    "train_pred_theta_avg = select_model_theta_avg.predict(new_X_train)\n",
    "pred_theta_avg = select_model_theta_avg.predict(new_X_test)\n",
    "print(rmsle(y_train_theta_avg, train_pred_theta_avg))\n",
    "\n",
    "train_pred_a_max = select_model_a_max.predict(new_X_train)\n",
    "pred_a_max = select_model_a_max.predict(new_X_test)\n",
    "print(rmsle(y_train_a_max, train_pred_a_max))\n",
    "\n",
    "train_pred_a_avg = select_model_a_avg.predict(new_X_train)\n",
    "pred_a_avg = select_model_a_avg.predict(new_X_test)\n",
    "print(rmsle(y_train_a_avg, train_pred_a_avg))\n",
    "\n",
    "train_pred_T1 = select_model_T1.predict(new_X_T1_train)\n",
    "pred_T1 = select_model_T1.predict(new_X_T1_test)\n",
    "print(rmsle(y_train_T1, train_pred_T1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "\n",
    "r2_theta_max = r2_score(y_test_theta_max, pred_theta_max)\n",
    "mse_theta_max = mean_squared_error(y_test_theta_max, pred_theta_max)\n",
    "mape_theta_max = mean_absolute_percentage_error(y_test_theta_max, pred_theta_max)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plot_regression_results(\n",
    "    ax, y_test_theta_max, pred_theta_max,\n",
    "    \"Stacking Regressor\",\n",
    "    (r'$R^2={:.2f}$' + '\\n' + r'$MAPE={:.2f}$' + '\\n' + r'$MSE={:.2f}$')\n",
    "    .format(r2_theta_max, mse_theta_max, mape_theta_max))\n",
    "\n",
    "r2_theta_avg = r2_score(y_test_theta_avg, pred_theta_avg)\n",
    "mse_theta_avg = mean_squared_error(y_test_theta_avg, pred_theta_avg)\n",
    "mape_theta_avg = mean_absolute_percentage_error(y_test_theta_avg, pred_theta_avg)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plot_regression_results(\n",
    "    ax, y_test_theta_avg, pred_theta_avg,\n",
    "    \"Stacking Regressor\",\n",
    "    (r'$R^2={:.2f}$' + '\\n' + r'$MAPE={:.2f}$' + '\\n' + r'$MSE={:.2f}$')\n",
    "    .format(r2_theta_avg, mse_theta_avg, mape_theta_avg))\n",
    "\n",
    "r2_a_max = r2_score(y_test_a_max, pred_a_max)\n",
    "mse_a_max = mean_squared_error(y_test_a_max, pred_a_max)\n",
    "mape_a_max = mean_absolute_percentage_error(y_test_a_max, pred_a_max)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plot_regression_results(\n",
    "    ax, y_test_a_max, pred_a_max,\n",
    "    \"Stacking Regressor\",\n",
    "    (r'$R^2={:.2f}$' + '\\n' + r'$MAPE={:.2f}$' + '\\n' + r'$MSE={:.2f}$')\n",
    "    .format(r2_a_max, mse_a_max, mape_a_max))\n",
    "\n",
    "r2_a_avg = r2_score(y_test_a_avg, pred_a_avg)\n",
    "mse_a_avg = mean_squared_error(y_test_a_avg, pred_a_avg)\n",
    "mape_a_avg = mean_absolute_percentage_error(y_test_a_avg, pred_a_avg)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plot_regression_results(\n",
    "    ax, y_test_a_avg, pred_a_avg,\n",
    "    \"Stacking Regressor\",\n",
    "    (r'$R^2={:.2f}$' + '\\n' + r'$MAPE={:.2f}$' + '\\n' + r'$MSE={:.2f}$')\n",
    "    .format(r2_a_avg, mse_a_avg, mape_a_avg))\n",
    "\n",
    "r2_T1 = r2_score(y_test_T1, pred_T1)\n",
    "mse_T1 = mean_squared_error(y_test_T1, pred_T1)\n",
    "mape_T1 = mean_absolute_percentage_error(y_test_T1, pred_T1)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plot_regression_results(\n",
    "    ax, y_test_T1, pred_T1,\n",
    "    \"Stacking Regressor\",\n",
    "    (r'$R^2={:.2f}$' + '\\n' + r'$MAPE={:.2f}$' + '\\n' + r'$MSE={:.2f}$')\n",
    "    .format(r2_T1, mse_T1, mape_T1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
